{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Learned Embeddings for Text.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPdgKagw6Y9Ql3IUcl6ZRZM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"gbvIxBTJyxzv"},"source":["# Import our standard libraries.\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns  # for nicer plots\n","sns.set(style='darkgrid')  # default style\n","import tensorflow as tf\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXq4Ys5kLM9A"},"source":["## Understanding the Embeddings Layer"]},{"cell_type":"code","metadata":{"id":"ZILRy2bDM81P"},"source":["# Input shape:  (batch_size, input_length)\n","# Output shape: (batch_size, input_length, output_dim)\n","embeddings = tf.keras.layers.Embedding(\n","    input_dim = 100,  # size of feature vocabulary\n","    output_dim = 2,   # embedding dimension\n","    input_length = 5  # number of inputs\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ftNkJwzN_Xu"},"source":["# Get embeddings for the input ids [0, 1, 2, 3, 4]\n","data = tf.constant([0, 1, 2, 3, 4], shape=(1, 5))\n","embed_data = embeddings(data)\n","embed_data.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHajItNaSD61"},"source":["# Average embeddings\n","embed_data_average = tf.keras.layers.GlobalAveragePooling1D()(embed_data)\n","embed_data_average.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uS1vgfpI3k89"},"source":["## Embeddings for Text"]},{"cell_type":"markdown","metadata":{"id":"2PGO-lK3J0R-"},"source":["Let's store our small set of movie reviews and their labels in numpy arrays"]},{"cell_type":"code","metadata":{"id":"8rFCBLrV4AQ3"},"source":["X_train = np.array([\n","                    'This movie was amazing',\n","                    'I have seen it 8 times !',\n","                    'I fell asleep',\n","                    'I would not recommend it',\n","                    'It was absolutely awful',\n","                    'I would watch it again !'\n","                  ])\n","\n","Y_train = np.array([\n","                    1,\n","                    1,\n","                    0,\n","                    0,  \n","                    0, \n","                    1\n","                  ])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__eF8JYS5WK5"},"source":["display(X_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8JGHiApe5fiY"},"source":["max_sequence_length = 6\n","\n","vectorize_layer = tf.keras.layers.TextVectorization(\n","    max_tokens=None, # Maximum vocabulary size - None -> no cap\n","    standardize='lower_and_strip_punctuation', # Standarization to apply to text - None -> no standarization\n","    split=\"whitespace\", # Values can be None (no splitting), \"whitespace\", or a Callable\n","    output_mode='int',  # Values can be \"int\", \"multi_hot\", \"count\" or \"tf_idf\"\n","    output_sequence_length=max_sequence_length, # Only valid in INT mode. If set, the output will have its time dimension padded or truncated to exactly output_sequence_length values\n","    )\n","\n","vectorize_layer.adapt(X_train)\n","\n","display(\"--Vocabulary--\")\n","for i, token in enumerate(vectorize_layer.get_vocabulary()):\n","  display('%d: %s' %(i, token))\n","\n","# 0: ('') - Padding Token\n","# 1: ('[UNK]') - OOV Token"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F1iNTJ7v-DfY"},"source":["X_train_vectorized = vectorize_layer(X_train)\n","\n","display(X_train_vectorized)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjqFXONSCtJI"},"source":["vocab_size = len(vectorize_layer.get_vocabulary())\n","\n","# Input shape:  (batch_size, input_length)\n","# Output shape: (batch_size, input_length, output_dim)\n","embedding_layer = tf.keras.layers.Embedding(\n","    input_dim = vocab_size,  # size of feature vocabulary\n","    output_dim = 3,   # embedding dimension\n","    input_length = max_sequence_length  # number of inputs\n","    )\n","\n","first_review_embed_rep = embedding_layer(X_train_vectorized[0])\n","display(first_review_embed_rep)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXiFgOojy5m-"},"source":["def build_model():\n","  tf.keras.backend.clear_session()\n","  tf.random.set_seed(0)\n","\n","  model = tf.keras.Sequential()\n","  model.add(vectorize_layer)\n","  model.add(tf.keras.layers.Embedding(\n","      input_dim = vocab_size,  # size of feature vocabulary\n","      output_dim = 2,  # embedding dimension\n","      input_length = max_sequence_length  # number of inputs\n","      ))\n","\n","  # Average over the sequence dimension, so each review is represented by \n","  # 1 vector of size embedding_dimension\n","  model.add(tf.keras.layers.GlobalAveragePooling1D()) \n","\n","  # Alternatively, we could concatenate the embedding representations of \n","  # all tokens in the movie review\n","  #model.add(tf.keras.layers.Flatten())\n","\n","  model.add(tf.keras.layers.Dense(\n","      units=8,        \n","      activation='relu'))\n","\n","  model.add(tf.keras.layers.Dense(\n","      units=1,        \n","      activation='sigmoid'))\n","\n","  model.compile(loss='binary_crossentropy', \n","                optimizer='adam',\n","                metrics=['accuracy'])\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38BFoq44G0bG"},"source":["model = build_model()\n","\n","# Display the model layers.\n","display(model.layers)\n","display(model.summary())\n","\n","# Retrieve the embeddings layer, which itself is wrapped in a list.\n","embeddings = model.layers[1].get_weights()[0]\n","display(\"Embeddings layer - shape: \", embeddings.shape)\n","display(\"Embeddings layer - parameter matrix (before training): \", embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MebyIPtzOApO"},"source":["def plot_embeddings(model):\n","  embeddings = model.layers[1].get_weights()[0]\n","  plt.scatter(embeddings[:,0], embeddings[:,1])\n","  for i, token in enumerate(vectorize_layer.get_vocabulary()):\n","    plt.annotate(token, (embeddings[i]))\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWVOJsbyJ53A"},"source":["plot_embeddings(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rmbLCqU_72d"},"source":["display(X_train)\n","display(model.predict(X_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOheiJzMIGrS"},"source":["history = model.fit(\n","  x = X_train,  # our sparse padded training data\n","  y = Y_train,  # corresponding binary labels\n","  epochs=15,    # number of passes through the training data\n","  verbose=0     # display some progress output during training\n","  )\n","\n","plot_embeddings(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MycKjnpTK5nh"},"source":["display(X_train)\n","display(model.predict(X_train))"],"execution_count":null,"outputs":[]}]}