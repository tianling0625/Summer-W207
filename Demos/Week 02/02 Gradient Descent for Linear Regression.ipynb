{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01 Gradient Descent for Linear Regression.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNOgo8Tn1aaBqOYARTcgIQP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Linear Regression (1 feature)"],"metadata":{"id":"qzfPOmyKvduy"}},{"cell_type":"code","metadata":{"id":"imcNmFXhPdCh","executionInfo":{"status":"ok","timestamp":1643090441398,"user_tz":480,"elapsed":3033,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}}},"source":["# Import our standard libraries.\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns  # for nicer plots\n","sns.set(style='darkgrid')  # default style\n","import tensorflow as tf"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VloE32t7dkU1"},"source":["## Data for Linear Regression\n","\n","Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$. Each datapoint represents the value of a single feature. Because we'll want our model to have a learned *bias* (or *intercept*), we will use an extra feature which will always be set to 1."]},{"cell_type":"code","metadata":{"id":"8bcduWsAbCRl","executionInfo":{"status":"ok","timestamp":1643090375033,"user_tz":480,"elapsed":3,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}}},"source":["# Here are our inputs.\n","X = np.array([[1, 3],\n","              [1, 2]])\n","Y = np.array([7, 5])"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSWDARd9eM99"},"source":["## Example slicing\n","Practice slicing the input array X."]},{"cell_type":"code","metadata":{"id":"IeQiqqBTd-t3","executionInfo":{"status":"ok","timestamp":1643090376249,"user_tz":480,"elapsed":5,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}}},"source":["# Use slicing to get the first X example (as an array)\n","x_e0 = X[0,:]\n","# Use slicing to get the second X example (as an array)\n","x_e1 = X[1,:]"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsuG8A-zeUV6"},"source":["## Make predictions\n","Suppose we are using linear regression. Then the functional form for our model is: \n","\n","\\begin{align}\n","h(x) &= w_0 + w_1x \n","\\end{align}\n","\n","Since we're using an extra feature corresponding to the intercept/bias, we can write:\n","\n","\\begin{align}\n","h(x) &= w_0x_0 + w_1x_1 \\\\\n","\\end{align}\n","\n","Given parameter values, practice computing model predictions."]},{"cell_type":"code","metadata":{"id":"HDbc2mGXcTvM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090379415,"user_tz":480,"elapsed":205,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"e12af905-78bf-472d-9911-b12df67d3490"},"source":["# Let's use a linear regression model: f(x) = w0 + w1*x1\n","W = np.array([1, 1])\n","\n","# Compute the prediction of the model for the first X example\n","pred_e0 = np.dot(W, X[0])\n","print(pred_e0)\n","\n","# Compute the prediction of the model for the second X example\n","pred_e1 = np.dot(W, X[1])\n","print(pred_e1)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n","3\n"]}]},{"cell_type":"markdown","metadata":{"id":"_tdGfEoDovBm"},"source":["Let's rewrite our model function with matrix multiplication and using the notation $h_W$ to make clear that our model is *parameterized* by $W$ (the vector of parameters):\n","\n","\\begin{align}\n","h_W(x) = w_0x_0 + w_1x_1 = xW^T\n","\\end{align}\n","\n","To make this matrix formulation as clear as possible, this is:\n","\n","\\begin{align}\n","\\hat{y} = h_W(x) = xW^T =\n","\\begin{pmatrix}\n","x_0 & x_1 \\\\\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","w_0 \\\\\n","w_1 \\\\\n","\\end{pmatrix}\n","\\end{align}\n","\n","In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n","\n","\\begin{align}\n","\\hat{Y} = h_W(X) = XW^T =\n","\\begin{pmatrix}\n","x_{0,0} & x_{0,1} \\\\\n","x_{1,0} & x_{1,1} \\\\\n","\\vdots & \\vdots \\\\\n","x_{m-1,0} & x_{m-1,1} \\\\\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","w_0 \\\\\n","w_1 \\\\\n","\\end{pmatrix}\n","\\end{align}\n","\n","Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up: \n","\n","\\begin{align}\n","X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n","\\end{align}\n","\n","Ok your turn.\n","\n","Use numpy functions to compute predictions for both X examples at once. The result should be a vector with 2 entries."]},{"cell_type":"code","metadata":{"id":"dRyOr1S1oWAx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090382824,"user_tz":480,"elapsed":318,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"f0172295-52f7-4d1a-f07d-3bc8217aa187"},"source":["# Compute predictions for all X examples at once.\n","#X: 2x2\n","#W.T: 2x1 \n","#Y_hat: 2x1\n","preds = np.dot(X, W.T)\n","print(preds)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[4 3]\n"]}]},{"cell_type":"markdown","metadata":{"id":"m8ZlvnYOpWxR"},"source":["## Loss\n","Use numpy functions to compute a vector of differences between model predictions and labels (values of Y)."]},{"cell_type":"code","metadata":{"id":"4UEd8aEbokKL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090384551,"user_tz":480,"elapsed":6,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"5b6a188c-2c8f-40e6-beaf-4a751f1f40dc"},"source":["# Compute differences between predictions and labels.\n","diff = preds - Y\n","print(diff)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[-3 -2]\n"]}]},{"cell_type":"markdown","metadata":{"id":"J90USA7GqA2b"},"source":["Now compute the MSE loss. The result should be a single (scalar) value. Remember we're using this formula (see assignment 1):\n","\n","\\begin{align}\n","J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n","\\end{align}\n","\n","where we've changed the standard scaling from $\\frac{1}{m}$ to $\\frac{1}{2m}$, where $m$ is the number of examples (2, in our case)."]},{"cell_type":"code","metadata":{"id":"TcjG93R6qe-H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090534135,"user_tz":480,"elapsed":198,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"e7bee7a8-3968-4b11-8b36-e4e674a313d0"},"source":["# Get the number of examples\n","m = X.shape[0]\n","\n","# Compute the average per-example loss\n","loss = np.sum(diff**2) / m\n","print(loss)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["6.5\n"]}]},{"cell_type":"markdown","metadata":{"id":"yWtjfoCZ5n1f"},"source":["## Gradient\n","\n","Refer to assignment 1 or the gradient descent lecture for the derivation, but here's the formula for the partial derivatives (which together form the gradient):\n","\n","\\begin{align}\n","\\frac{\\partial}{\\partial w_j} J(W) &= (h_W(x) - y)x_j\n","\\end{align}\n","\n","This formula is assuming we only have a single example. The general formula has an average over examples:\n","\n","\\begin{align}\n","\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{1}{m}\\sum_i(h_W(x^{(i)}) - y^{(i)})x^{(i)}_j\n","\\end{align}\n","\n","You're ready to compute the gradient. The result will be a vector of partial derivatives for $w_0$ and $w_1$ respectively. You can use matrix computations as before.\n","\n","\\begin{align}\n","\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)X\n","\\end{align}"]},{"cell_type":"code","metadata":{"id":"EwxFEpUr5q9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090387372,"user_tz":480,"elapsed":166,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"5e4cc30e-880f-4458-d5be-5fd53a5a8d90"},"source":["# compute the gradient\n","gradient = np.dot(diff, X) / m\n","print(gradient)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[-2.5 -6.5]\n"]}]},{"cell_type":"markdown","metadata":{"id":"CpBEnP5sLZks"},"source":["## Parameter updates\n","Now that you've computed the gradient, apply parameter updates by subtracting the appropriate partial derivatives (scaled by a learning rate) from the initial parameter values.\n"]},{"cell_type":"code","metadata":{"id":"5dtpDEhYZZwg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090389127,"user_tz":480,"elapsed":5,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"48c10deb-a0b0-4d07-ea03-1791602046c9"},"source":["# Update parameter values\n","learning_rate = 0.1\n","W = W - learning_rate*gradient\n","print(W)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.25 1.65]\n"]}]},{"cell_type":"markdown","source":["# Linear Regression (multiple features)\n","\n","Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$, but now we have multiple features."],"metadata":{"id":"ff3KjlpLvsbG"}},{"cell_type":"code","metadata":{"id":"CVGGtopiv39T","executionInfo":{"status":"ok","timestamp":1643090391819,"user_tz":480,"elapsed":163,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}}},"source":["# Here are our inputs.\n","X = np.array([[1, 3, 1, 1],\n","              [1, 2, 2, 0]])\n","Y = np.array([2, 1])"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4bG4AeHv39T"},"source":["Let's write out our model function:\n","\n","\\begin{align}\n","h_W(x) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 = xW^T\n","\\end{align}\n","\n","We can get all predictions with this matrix product:\n","\n","\\begin{align}\n","\\hat{Y} = h_W(X) = XW^T =\n","\\begin{pmatrix}\n","x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n","x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n","\\vdots & \\vdots & \\vdots & \\vdots \\\\\n","x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","w_0 \\\\\n","w_1 \\\\\n","w_2 \\\\\n","w_3 \\\\\n","\\end{pmatrix}\n","\\end{align}\n","\n","Given the (initial) parameter values below, compute the predictions."]},{"cell_type":"code","metadata":{"id":"pGg1Ll4I4jR6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090393439,"user_tz":480,"elapsed":168,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"d404bdac-1104-4983-b597-cf0f03247e5d"},"source":["# Initial parameter values.\n","W = [1, 1, 1, 1]\n","\n","# Compute predictions.\n","preds = np.dot(X, W)\n","print(preds)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[6 5]\n"]}]},{"cell_type":"markdown","metadata":{"id":"zZ_pD-j2_qzV"},"source":["Now compute the gradient."]},{"cell_type":"code","metadata":{"id":"T-Sam58B8B1_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090395232,"user_tz":480,"elapsed":163,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"0bd0b1d1-715c-4fc6-911e-4e8220abb223"},"source":["m, n = X.shape\n","gradient = np.dot((preds - Y), X) / m\n","print(gradient)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 4. 10.  6.  2.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"PmYUmEjg_sk8"},"source":["And now put everything together in gradient descent. You can run this cell repeatedly."]},{"cell_type":"code","metadata":{"id":"Zl_Nu_wB8ar4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090413994,"user_tz":480,"elapsed":153,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"6c196a39-9200-43e3-e090-80950c76373b"},"source":["# Run gradient descent\n","learning_rate = 0.1\n","\n","preds = np.dot(X, W)\n","loss = ((preds - Y)**2).mean()\n","gradient = 2 * np.dot((preds - Y), X) / m\n","W = W - learning_rate * gradient\n","\n","print('predictions:', preds)\n","print('loss:', loss)\n","print('gradient:', gradient)\n","print('weights:', W)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["predictions: [6 5]\n","loss: 16.0\n","gradient: [ 8. 20. 12.  4.]\n","weights: [ 0.2 -1.  -0.2  0.6]\n"]}]},{"cell_type":"markdown","metadata":{"id":"BMsSJ-ZD_12e"},"source":["## Now with TensorFlow/Keras"]},{"cell_type":"code","metadata":{"id":"jisaFtGY__KL","executionInfo":{"status":"ok","timestamp":1643090447058,"user_tz":480,"elapsed":344,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}}},"source":["tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Dense(\n","    units=1,                     # output dim\n","    input_shape=[4],             # input dim\n","    use_bias=False,              # we included the bias in X\n","    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n","))\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n","model.compile(loss='mse', optimizer=optimizer)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PQ-RDwXCKVt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643090453285,"user_tz":480,"elapsed":1316,"user":{"displayName":"Daniel Gillick","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64","userId":"01872965353911650729"}},"outputId":"ca4ff499-ec50-4afc-d654-94747a8d6a59"},"source":["history = model.fit(\n","  x = X,\n","  y = Y,\n","  epochs=1,\n","  batch_size=2,\n","  verbose=0)\n","loss = history.history['loss'][0]\n","weights = model.layers[0].get_weights()[0].T\n","preds = model.predict(X)\n","\n","print('predictions:', preds.T)\n","print('loss:', loss)\n","print('W:', weights)"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["predictions: [[-2.4 -2.2]]\n","loss: 16.0\n","W: [[ 0.19999999 -1.         -0.20000002  0.6       ]]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"-QZEAyrD0pyz"},"execution_count":null,"outputs":[]}]}