{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "imcNmFXhPdCh"
   },
   "outputs": [],
   "source": [
    "# Import our standard libraries.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style='darkgrid')  # default style\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=3, suppress=True)  # improve float readability\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4mROCY5wAX4"
   },
   "source": [
    "## Iris Classification\n",
    "\n",
    "We will train a classifier to predict 3 iris varieties from 4 features of each flower. Note: we are not doing image classification here!\n",
    "\n",
    "![An image](https://drive.google.com/uc?id=12gf4Q0K45gvw-tUDt_sWsbAl-f0klhib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "37XEUjK4ulzp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4)\n",
      "Y shape: (150,)\n",
      "feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "class names: ['setosa' 'versicolor' 'virginica']\n",
      "First example: [5.1 3.5 1.4 0.2] 0\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('Y shape:', Y.shape)\n",
    "print('feature names:', feature_names)\n",
    "print('class names:', class_names)\n",
    "print('First example:', X[0], Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3GC13Sf219q"
   },
   "source": [
    "## Data Processing\n",
    "\n",
    "* Shuffle\n",
    "* Split into train/test\n",
    "* Apply mean and variance normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-sa_lrwU1oiT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "shuffled_indices = np.random.permutation(range(len(Y)))\n",
    "X = X[shuffled_indices]\n",
    "Y = Y[shuffled_indices]\n",
    "\n",
    "X_train = X[0:100]\n",
    "Y_train = Y[0:100]\n",
    "X_test = X[100:150]\n",
    "Y_test = Y[100:150]\n",
    "\n",
    "X_train_means = np.mean(X_train, axis=0)\n",
    "X_train_stds = np.std(X_train, axis=0)\n",
    "X_train = (X_train - X_train_means) / X_train_stds\n",
    "X_test = (X_test - X_train_means)/ X_train_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jIgCYbiVAz3"
   },
   "source": [
    "## Sparse vs Dense Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8bcduWsAbCRl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Convert Y from sparse to dense if needed\n",
    "# one-hot [0, 0, 1] -> 2\n",
    "# one-hot [0, 1, 0] -> 1\n",
    "# one-hot [1, 0, 0] -> 0\n",
    "Y_train_dense = tf.keras.utils.to_categorical(Y_train)\n",
    "print(Y_train_dense.shape)\n",
    "print(Y_train_dense[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS7LIrIlVd2E"
   },
   "source": [
    "## Softmax Regression Functional Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tdGfEoDovBm"
   },
   "source": [
    "We will use *softmax regression*, which extends *logistic regression* to the multiclass setting. Our model will make predictions for input examples $X$ by:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h_W(X) = \\phi(XW^T) =\n",
    "\\phi\\begin{pmatrix}\n",
    "x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_{0,0} & w_{1,0} & w_{2,0} \\\\\n",
    "w_{0,1} & w_{1,1} & w_{2,1} \\\\\n",
    "w_{0,2} & w_{1,2} & w_{2,2} \\\\\n",
    "w_{0,3} & w_{1,3} & w_{2,3} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "A few notes about this computation:\n",
    "\n",
    "* Our X has shape (100 x 4): 100 examples and 4 features\n",
    "* Our W has shape (3 x 4): 3 classes and 4 features. The indices above are reversed because we've taken the transpose of W: the first column of $W^T$ contains the weights for the first class.\n",
    "* The result will have shape (100 x 3): 3 probabilities corresponding to the 3 classes for each of the 100 examples.\n",
    "* $\\phi$ is the softmax function: $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$. It is applied to the rows of $XW^T$.\n",
    "\n",
    "More detailed background [here](http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAoIx-nkXhD-"
   },
   "source": [
    "## Softmax Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Hpah13BcCVXo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09  0.245 0.665]\n",
      " [0.016 0.117 0.867]]\n"
     ]
    }
   ],
   "source": [
    "# Remember the sigmoid function.\n",
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Our softmax function will normalize over the rows of the input matrix.\n",
    "def softmax(z):\n",
    "  \"\"\"z has shape (m, n): examples, classes\"\"\"\n",
    "  (m, n) = z.shape\n",
    "\n",
    "  # First exponentiate each value\n",
    "  exps = np.exp(z)\n",
    "\n",
    "  # Get the sum of each row and normalize\n",
    "  row_sums = np.sum(exps, axis=1)\n",
    "  for i in range(m):\n",
    "    exps[i,:] /= row_sums[i]\n",
    "  \n",
    "  # Fancy/tricky way to do row-wise sums in numpy:\n",
    "  # return np.divide(exps.T, np.sum(exps, axis=1)).T\n",
    "\n",
    "  return exps\n",
    "\n",
    "# Try an example.\n",
    "v = np.array([[1,2,3],\n",
    "              [0,2,4]])\n",
    "print(softmax(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLh6VWUfGm7_"
   },
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now, given some initial parameter values (below), compute the model's initial predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "pGg1Ll4I4jR6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]]\n",
      "label predictions:\n",
      " [0 0 0 0 0 0]\n",
      "true labels:\n",
      " [2 1 0 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameter values.\n",
    "# W = np.random.uniform(size=(3,4))\n",
    "W = np.ones((3,4))\n",
    "\n",
    "# Compute predictions.\n",
    "preds = softmax(np.dot(X_train, W.T))\n",
    "print('predictions:\\n', preds[:6])\n",
    "print('label predictions:\\n', np.argmax(preds, axis=1)[:6])\n",
    "print('true labels:\\n', Y_train[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIbpXB4ZHPvO"
   },
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "We'll use the general form of *cross-entropy* loss:\n",
    "\n",
    "\\begin{align}\n",
    "CrossEntropyLoss = \\frac{1}{m} \\sum_i \\sum_j -y_j\\log(\\hat{y_j})\n",
    "\\end{align}\n",
    "\n",
    "In this formula:\n",
    "\n",
    "* $j$ indexes the classes (in our case [0,1,2]) and each $y$ has a dense representation like [0,0,1] which indicates class 2.\n",
    "* *i* indexes over training examples, so we're computing an average loss (as usual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lWxpr2OogN70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986122886681093\n"
     ]
    }
   ],
   "source": [
    "def ce_loss(preds, Y):\n",
    "  \"\"\"\n",
    "    preds are (m,n) m = number of examples, n = number of classes\n",
    "    Y is (m,) -- array of sparse labels \n",
    "    preds[0] = [.1, .1, .8] Y[0] = 2 Y_dense[0] = [0, 0, 1]\n",
    "  \"\"\"\n",
    "  # Get the number of examples\n",
    "  m = Y.shape[0]\n",
    "\n",
    "  # Compute the first sum, the cross-entropy for each example, using\n",
    "  # the rows of the predictions and corresponding labels.\n",
    "  # Note that we need the dense (one-hot) labels.\n",
    "  Y_dense = tf.keras.utils.to_categorical(Y)\n",
    "  # [.1, .1, .8] [0, 0, 1] -> [0, 0, -1*log(.8)] -> -1*log(.8)\n",
    "  cross_entropy_values = - np.sum(Y_dense * np.log(preds), axis=1)\n",
    "\n",
    "  # Here's a more efficient but tricky way to do this:\n",
    "  # cross_entropy_values = -np.log(preds[range(m), Y])\n",
    "\n",
    "  # Sum the per-example cross-entropy values.\n",
    "  loss = np.sum(cross_entropy_values) / m\n",
    "\n",
    "  return loss\n",
    "\n",
    "#print(ce_loss(np.array([.1, .1, .8]), np.array([2])))\n",
    "print(ce_loss(preds, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaRg8b1F93w9"
   },
   "source": [
    "## Computing the Gradient\n",
    "\n",
    "Again, it will turn out that the gradient computation is the same as it was for MSE with linear regression. A happy coincidence.\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)^TX\n",
    "\\end{align}\n",
    "\n",
    "Remember that our parameters $W$ are represented by a matrix of shape (3 x 4): 3 classes and 4 features. The gradient will include a partial derivative for every parameter, and is an average over gradients computed on each training example.\n",
    "\n",
    "Let's review the matrix shapes:\n",
    "\n",
    "* $h_W(X)$ is (100 x 3): 3 probabilities for each example.\n",
    "* $Y$ is (100 x 3): this is the dense (one-hot) version of the labels, matching the shape of the predictions.\n",
    "* $X$ is (100 x 4): 4 features for each example.\n",
    "* The resulting product is (3 x 100)(100 x 4), giving a (3 x 4) output, which matches the shape of our parameters $W$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0-j0soKK2qfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient:\n",
      " [[ 0.337 -0.28   0.431  0.411]\n",
      " [-0.042  0.191 -0.089 -0.046]\n",
      " [-0.295  0.09  -0.342 -0.365]]\n"
     ]
    }
   ],
   "source": [
    "# y' = [.1, .2, .7]  y = [0, 0, 1]  diff = y' - y = [.1, .2, -.3]\n",
    "# d1 = [.1, .2, -.3]  x1 = [1, 2, 3, 4]\n",
    "# (3 x 100) (100 x 4) -> (3 x 4)\n",
    "# [ [ .1*1,  .1*2,  .1*3,  .1*4 ]\n",
    "#   [ .2*1,  .2*2,  .2*3,  .2*4 ]\n",
    "#   [-.3*1, -.3*2, -.3*3, -.3*4 ]\n",
    "# ]\n",
    "#\n",
    "# We need the dense version of Y here\n",
    "m = Y_train.shape[0]\n",
    "Y_train_dense = tf.keras.utils.to_categorical(Y_train)\n",
    "diff = preds - Y_train_dense\n",
    "gradient = np.dot(diff.T, X_train) / m\n",
    "print('gradient:\\n', gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ExL4G-pMAXvV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.333]\n",
      " [ 0.333]\n",
      " [-0.667]]\n",
      "[[-0.017 -0.543  0.76   1.567]]\n",
      "gradient:\n",
      " [[-0.006 -0.181  0.253  0.522]\n",
      " [-0.006 -0.181  0.253  0.522]\n",
      " [ 0.011  0.362 -0.507 -1.045]]\n"
     ]
    }
   ],
   "source": [
    "# Simplify and just compute the gradient for the first training example.\n",
    "print(diff[0:1].T)\n",
    "print(X_train[0:1])\n",
    "print('gradient:\\n', np.dot(diff[0:1].T, X_train[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZDyrbc42rcF"
   },
   "source": [
    "## Running Gradient Descent\n",
    "\n",
    "Let's put together the code for a single gradient descent step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Zl_Nu_wB8ar4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      " [2 1 0 2 0 2]\n",
      "predictions:\n",
      " [[0.025 0.201 0.774]\n",
      " [0.084 0.673 0.243]\n",
      " [0.99  0.006 0.003]\n",
      " [0.007 0.154 0.838]\n",
      " [0.962 0.032 0.006]\n",
      " [0.014 0.081 0.905]]\n",
      "loss: 0.43657251861677077\n",
      "gradient:\n",
      " [[ 0.012 -0.026  0.026  0.023]\n",
      " [-0.01   0.026 -0.007  0.01 ]\n",
      " [-0.001 -0.    -0.018 -0.033]]\n",
      "weights:\n",
      " [[0.539 1.556 0.295 0.347]\n",
      " [1.08  0.492 1.132 0.922]\n",
      " [1.381 0.951 1.573 1.731]]\n"
     ]
    }
   ],
   "source": [
    "# Run gradient descent\n",
    "m, n = X.shape  # m = number of examples; n = number of features (including bias)\n",
    "learning_rate = 0.01\n",
    "\n",
    "for _ in range(1000):\n",
    "  preds = softmax(np.dot(X_train, W.T))\n",
    "  loss = ce_loss(preds, Y_train)\n",
    "  gradient = np.dot((preds - tf.keras.utils.to_categorical(Y_train)).T, X_train) / m\n",
    "  W = W - learning_rate * gradient\n",
    "\n",
    "print('labels:\\n', Y_train[:6])\n",
    "print('predictions:\\n', preds[:6])\n",
    "print('loss:', loss)\n",
    "print('gradient:\\n', gradient)\n",
    "print('weights:\\n', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q72Tu_n_LlO"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tj3z7t6-_PZ4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "test_preds = softmax(np.dot(X_test, W.T))\n",
    "test_pred_labels = np.argmax(test_preds, axis=1)\n",
    "print('Accuracy:', np.mean(test_pred_labels == Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xcq8zqKDALmC"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf = tf.math.confusion_matrix(Y_test, test_pred_labels)\n",
    "ax = sns.heatmap(cf, annot=True, fmt='.3g', cmap='Blues',\n",
    "                 xticklabels=class_names, yticklabels=class_names, cbar=False)\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMsSJ-ZD_12e"
   },
   "source": [
    "## Now with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jisaFtGY__KL"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=3,                     # output dim\n",
    "    input_shape=[4],             # input dim\n",
    "    use_bias=False,              # we included the bias in X\n",
    "    activation='softmax',        # apply a sigmoid to the output\n",
    "    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
    "))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3PQ-RDwXCKVt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      " [[0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]]\n",
      "loss: 0.9866025447845459\n",
      "W:\n",
      " [[ 0.247  2.903 -0.789 -0.718]\n",
      " [ 1.606 -0.277  2.128  0.136]\n",
      " [ 1.201  0.611  2.037  2.975]]\n"
     ]
    }
   ],
   "source": [
    "# As above, get predictions for the current model first.\n",
    "preds = model.predict(X)\n",
    "\n",
    "# Do a single gradient update.\n",
    "history = model.fit(\n",
    "  x = X_train,\n",
    "  y = Y_train,\n",
    "  epochs=100,\n",
    "  batch_size=10,\n",
    "  verbose=0)\n",
    "\n",
    "# Show the loss (before the update) and the new weights.\n",
    "loss = history.history['loss'][0]\n",
    "weights = model.layers[0].get_weights()[0].T\n",
    "print('predictions:\\n', preds[:6])\n",
    "print('loss:', loss)\n",
    "print('W:\\n', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QAmb6PMCTVET"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9866025447845459, 0.8018091320991516, 0.6731857061386108, 0.5967690944671631, 0.5471956133842468, 0.514506459236145, 0.49091407656669617, 0.47425833344459534, 0.46118542551994324, 0.4509076774120331, 0.441916286945343, 0.4335900843143463, 0.4278513193130493, 0.42193564772605896, 0.4161439538002014, 0.4112893044948578, 0.40704959630966187, 0.40306878089904785, 0.40030038356781006, 0.3961119055747986, 0.39336758852005005, 0.3906244933605194, 0.3878195285797119, 0.38501647114753723, 0.3831064701080322, 0.3820681869983673, 0.3788335919380188, 0.3768366575241089, 0.3748217821121216, 0.37313511967658997, 0.37181004881858826, 0.37028777599334717, 0.36910170316696167, 0.36707618832588196, 0.36581456661224365, 0.36528006196022034, 0.364219069480896, 0.3626328408718109, 0.36160263419151306, 0.3611098527908325, 0.3594171404838562, 0.3584742248058319, 0.3578779101371765, 0.3571168780326843, 0.3559262156486511, 0.35589736700057983, 0.35451415181159973, 0.35403603315353394, 0.3537735044956207, 0.35254982113838196, 0.35208579897880554, 0.35116124153137207, 0.35110533237457275, 0.3508851230144501, 0.34955352544784546, 0.34926730394363403, 0.34867939352989197, 0.34796634316444397, 0.3476303517818451, 0.34705424308776855, 0.3473154902458191, 0.3467359244823456, 0.34636828303337097, 0.3466026782989502, 0.34531936049461365, 0.3451850116252899, 0.34413790702819824, 0.3440379202365875, 0.3436696231365204, 0.3439505100250244, 0.3428983986377716, 0.3424402177333832, 0.3420674204826355, 0.3420584201812744, 0.341478168964386, 0.34149134159088135, 0.3409688174724579, 0.34084799885749817, 0.34227821230888367, 0.34045618772506714, 0.33992743492126465, 0.3399190604686737, 0.3393455147743225, 0.3397267460823059, 0.33894744515419006, 0.339199036359787, 0.33842697739601135, 0.3381493091583252, 0.33824974298477173, 0.33808568120002747, 0.3375113606452942, 0.3373515009880066, 0.33720630407333374, 0.3373667001724243, 0.33711737394332886, 0.3364616632461548, 0.3363942801952362, 0.33639752864837646, 0.33604884147644043, 0.33574140071868896]\n"
     ]
    }
   ],
   "source": [
    "print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DuKK7l4fTktl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25620391964912415"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/popularity_contest/reporter.py\", line 105, in report_popularity\n",
      "    libraries = get_used_libraries(initial_modules, current_modules)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/popularity_contest/reporter.py\", line 74, in get_used_libraries\n",
      "    all_packages = get_all_packages()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/popularity_contest/reporter.py\", line 51, in get_all_packages\n",
      "    for f in dist.files:\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "test_preds_labels = np.argmax(test_preds, axis=1)\n",
    "accuracy = np.mean(test_preds_labels == Y_test)\n",
    "print(accuracy)\n",
    "model.evaluate(x=X_test, y=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOF/SzfGqGLnE58b8QnQuUU",
   "name": "03 Multiclass Logistic Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
