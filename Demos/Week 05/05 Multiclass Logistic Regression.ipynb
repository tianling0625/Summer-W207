{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03 Multiclass Logistic Regression.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOF/SzfGqGLnE58b8QnQuUU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"imcNmFXhPdCh"},"source":["# Import our standard libraries.\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns  # for nicer plots\n","sns.set(style='darkgrid')  # default style\n","import tensorflow as tf\n","np.set_printoptions(precision=3, suppress=True)  # improve float readability\n","from sklearn import datasets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P4mROCY5wAX4"},"source":["## Iris Classification\n","\n","We will train a classifier to predict 3 iris varieties from 4 features of each flower. Note: we are not doing image classification here!\n","\n","![An image](https://drive.google.com/uc?id=12gf4Q0K45gvw-tUDt_sWsbAl-f0klhib)\n"]},{"cell_type":"code","metadata":{"id":"37XEUjK4ulzp"},"source":["# Load the data\n","iris = datasets.load_iris()\n","X = iris.data\n","Y = iris.target\n","feature_names = iris.feature_names\n","class_names = iris.target_names\n","\n","print('X shape:', X.shape)\n","print('Y shape:', Y.shape)\n","print('feature names:', feature_names)\n","print('class names:', class_names)\n","print('First example:', X[0], Y[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3GC13Sf219q"},"source":["## Data Processing\n","\n","* Shuffle\n","* Split into train/test\n","* Apply mean and variance normalization"]},{"cell_type":"code","metadata":{"id":"-sa_lrwU1oiT"},"source":["np.random.seed(0)\n","shuffled_indices = np.random.permutation(range(len(Y)))\n","X = X[shuffled_indices]\n","Y = Y[shuffled_indices]\n","\n","X_train = X[0:100]\n","Y_train = Y[0:100]\n","X_test = X[100:150]\n","Y_test = Y[100:150]\n","\n","X_train_means = np.mean(X_train, axis=0)\n","X_train_stds = np.std(X_train, axis=0)\n","X_train = (X_train - X_train_means) / X_train_stds\n","X_test = (X_test - X_train_means)/ X_train_stds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jIgCYbiVAz3"},"source":["## Sparse vs Dense Representation"]},{"cell_type":"code","metadata":{"id":"8bcduWsAbCRl"},"source":["# Convert Y from sparse to dense if needed\n","# one-hot [0, 0, 1] -> 2\n","# one-hot [0, 1, 0] -> 1\n","# one-hot [1, 0, 0] -> 0\n","Y_train_dense = tf.keras.utils.to_categorical(Y_train)\n","print(Y_train_dense.shape)\n","print(Y_train_dense[:6])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FS7LIrIlVd2E"},"source":["## Softmax Regression Functional Form"]},{"cell_type":"markdown","metadata":{"id":"_tdGfEoDovBm"},"source":["We will use *softmax regression*, which extends *logistic regression* to the multiclass setting. Our model will make predictions for input examples $X$ by:\n","\n","\\begin{align}\n","\\hat{Y} = h_W(X) = \\phi(XW^T) =\n","\\phi\\begin{pmatrix}\n","x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n","x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n","\\vdots & \\vdots & \\vdots & \\vdots \\\\\n","x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","w_{0,0} & w_{1,0} & w_{2,0} \\\\\n","w_{0,1} & w_{1,1} & w_{2,1} \\\\\n","w_{0,2} & w_{1,2} & w_{2,2} \\\\\n","w_{0,3} & w_{1,3} & w_{2,3} \\\\\n","\\end{pmatrix}\n","\\end{align}\n","\n","A few notes about this computation:\n","\n","* Our X has shape (100 x 4): 100 examples and 4 features\n","* Our W has shape (3 x 4): 3 classes and 4 features. The indices above are reversed because we've taken the transpose of W: the first column of $W^T$ contains the weights for the first class.\n","* The result will have shape (100 x 3): 3 probabilities corresponding to the 3 classes for each of the 100 examples.\n","* $\\phi$ is the softmax function: $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$. It is applied to the rows of $XW^T$.\n","\n","More detailed background [here](http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/)."]},{"cell_type":"markdown","metadata":{"id":"nAoIx-nkXhD-"},"source":["## Softmax Normalization"]},{"cell_type":"code","metadata":{"id":"Hpah13BcCVXo"},"source":["# Remember the sigmoid function.\n","def sigmoid(z):\n","  return 1 / (1 + np.exp(-z))\n","\n","# Our softmax function will normalize over the rows of the input matrix.\n","def softmax(z):\n","  \"\"\"z has shape (m, n): examples, classes\"\"\"\n","  (m, n) = z.shape\n","\n","  # First exponentiate each value\n","  exps = np.exp(z)\n","\n","  # Get the sum of each row and normalize\n","  row_sums = np.sum(exps, axis=1)\n","  for i in range(m):\n","    exps[i,:] /= row_sums[i]\n","  \n","  # Fancy/tricky way to do row-wise sums in numpy:\n","  # return np.divide(exps.T, np.sum(exps, axis=1)).T\n","\n","  return exps\n","\n","# Try an example.\n","v = np.array([[1,2,3],\n","              [0,2,4]])\n","print(softmax(v))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLh6VWUfGm7_"},"source":["## Making Predictions\n","\n","Now, given some initial parameter values (below), compute the model's initial predictions."]},{"cell_type":"code","metadata":{"id":"pGg1Ll4I4jR6"},"source":["# Initial parameter values.\n","# W = np.random.uniform(size=(3,4))\n","W = np.ones((3,4))\n","\n","# Compute predictions.\n","preds = softmax(np.dot(X_train, W.T))\n","print('predictions:\\n', preds[:6])\n","print('label predictions:\\n', np.argmax(preds, axis=1)[:6])\n","print('true labels:\\n', Y_train[:6])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIbpXB4ZHPvO"},"source":["## Cross-Entropy Loss\n","\n","We'll use the general form of *cross-entropy* loss:\n","\n","\\begin{align}\n","CrossEntropyLoss = \\frac{1}{m} \\sum_i \\sum_j -y_j\\log(\\hat{y_j})\n","\\end{align}\n","\n","In this formula:\n","\n","* $j$ indexes the classes (in our case [0,1,2]) and each $y$ has a dense representation like [0,0,1] which indicates class 2.\n","* *i* indexes over training examples, so we're computing an average loss (as usual)."]},{"cell_type":"code","metadata":{"id":"lWxpr2OogN70"},"source":["def ce_loss(preds, Y):\n","  \"\"\"\n","    preds are (m,n) m = number of examples, n = number of classes\n","    Y is (m,) -- array of sparse labels \n","    preds[0] = [.1, .1, .8] Y[0] = 2 Y_dense[0] = [0, 0, 1]\n","  \"\"\"\n","  # Get the number of examples\n","  m = Y.shape[0]\n","\n","  # Compute the first sum, the cross-entropy for each example, using\n","  # the rows of the predictions and corresponding labels.\n","  # Note that we need the dense (one-hot) labels.\n","  Y_dense = tf.keras.utils.to_categorical(Y)\n","  # [.1, .1, .8] [0, 0, 1] -> [0, 0, -1*log(.8)] -> -1*log(.8)\n","  cross_entropy_values = - np.sum(Y_dense * np.log(preds), axis=1)\n","\n","  # Here's a more efficient but tricky way to do this:\n","  # cross_entropy_values = -np.log(preds[range(m), Y])\n","\n","  # Sum the per-example cross-entropy values.\n","  loss = np.sum(cross_entropy_values) / m\n","\n","  return loss\n","\n","#print(ce_loss(np.array([.1, .1, .8]), np.array([2])))\n","print(ce_loss(preds, Y_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BaRg8b1F93w9"},"source":["## Computing the Gradient\n","\n","Again, it will turn out that the gradient computation is the same as it was for MSE with linear regression. A happy coincidence.\n","\n","\\begin{align}\n","\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)^TX\n","\\end{align}\n","\n","Remember that our parameters $W$ are represented by a matrix of shape (3 x 4): 3 classes and 4 features. The gradient will include a partial derivative for every parameter, and is an average over gradients computed on each training example.\n","\n","Let's review the matrix shapes:\n","\n","* $h_W(X)$ is (100 x 3): 3 probabilities for each example.\n","* $Y$ is (100 x 3): this is the dense (one-hot) version of the labels, matching the shape of the predictions.\n","* $X$ is (100 x 4): 4 features for each example.\n","* The resulting product is (3 x 100)(100 x 4), giving a (3 x 4) output, which matches the shape of our parameters $W$.\n"]},{"cell_type":"code","metadata":{"id":"0-j0soKK2qfc"},"source":["# y' = [.1, .2, .7]  y = [0, 0, 1]  diff = y' - y = [.1, .2, -.3]\n","# d1 = [.1, .2, -.3]  x1 = [1, 2, 3, 4]\n","# (3 x 100) (100 x 4) -> (3 x 4)\n","# [ [ .1*1,  .1*2,  .1*3,  .1*4 ]\n","#   [ .2*1,  .2*2,  .2*3,  .2*4 ]\n","#   [-.3*1, -.3*2, -.3*3, -.3*4 ]\n","# ]\n","#\n","# We need the dense version of Y here\n","m = Y_train.shape[0]\n","Y_train_dense = tf.keras.utils.to_categorical(Y_train)\n","diff = preds - Y_train_dense\n","gradient = np.dot(diff.T, X_train) / m\n","print('gradient:\\n', gradient)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ExL4G-pMAXvV"},"source":["# Simplify and just compute the gradient for the first training example.\n","print(diff[0:1].T)\n","print(X_train[0:1])\n","print('gradient:\\n', np.dot(diff[0:1].T, X_train[0:1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZDyrbc42rcF"},"source":["## Running Gradient Descent\n","\n","Let's put together the code for a single gradient descent step:"]},{"cell_type":"code","metadata":{"id":"Zl_Nu_wB8ar4"},"source":["# Run gradient descent\n","m, n = X.shape  # m = number of examples; n = number of features (including bias)\n","learning_rate = 0.01\n","\n","for _ in range(1000):\n","  preds = softmax(np.dot(X_train, W.T))\n","  loss = ce_loss(preds, Y_train)\n","  gradient = np.dot((preds - tf.keras.utils.to_categorical(Y_train)).T, X_train) / m\n","  W = W - learning_rate * gradient\n","\n","print('labels:\\n', Y_train[:6])\n","print('predictions:\\n', preds[:6])\n","print('loss:', loss)\n","print('gradient:\\n', gradient)\n","print('weights:\\n', W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8q72Tu_n_LlO"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"tj3z7t6-_PZ4"},"source":["# Make predictions on the test data\n","test_preds = softmax(np.dot(X_test, W.T))\n","test_pred_labels = np.argmax(test_preds, axis=1)\n","print('Accuracy:', np.mean(test_pred_labels == Y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcq8zqKDALmC"},"source":["cf = tf.math.confusion_matrix(Y_test, test_pred_labels)\n","ax = sns.heatmap(cf, annot=True, fmt='.3g', cmap='Blues',\n","                 xticklabels=class_names, yticklabels=class_names, cbar=False)\n","ax.set(xlabel='Predicted Label', ylabel='True Label')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMsSJ-ZD_12e"},"source":["## Now with TensorFlow/Keras"]},{"cell_type":"code","metadata":{"id":"jisaFtGY__KL"},"source":["tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Dense(\n","    units=3,                     # output dim\n","    input_shape=[4],             # input dim\n","    use_bias=False,              # we included the bias in X\n","    activation='softmax',        # apply a sigmoid to the output\n","    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n","))\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PQ-RDwXCKVt"},"source":["# As above, get predictions for the current model first.\n","preds = model.predict(X)\n","\n","# Do a single gradient update.\n","history = model.fit(\n","  x = X_train,\n","  y = Y_train,\n","  epochs=100,\n","  batch_size=10,\n","  verbose=0)\n","\n","# Show the loss (before the update) and the new weights.\n","loss = history.history['loss'][0]\n","weights = model.layers[0].get_weights()[0].T\n","print('predictions:\\n', preds[:6])\n","print('loss:', loss)\n","print('W:\\n', weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QAmb6PMCTVET"},"source":["print(history.history['loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DuKK7l4fTktl"},"source":["test_preds = model.predict(X_test)\n","test_preds_labels = np.argmax(test_preds, axis=1)\n","accuracy = np.mean(test_preds_labels == Y_test)\n","print(accuracy)\n","model.evaluate(x=X_test, y=Y_test)"],"execution_count":null,"outputs":[]}]}