{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02 Gradient Descent for Logistic Regression.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPYfdglyDDTfD4jgFZaisB0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"imcNmFXhPdCh"},"source":["# Import our standard libraries.\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns  # for nicer plots\n","sns.set(style='darkgrid')  # default style\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VloE32t7dkU1"},"source":["## Logistic Regression\n","\n","Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$, each with 3 features (and a dummy 1 for learning the bias). Now our target labels are binary (0 or 1)."]},{"cell_type":"code","metadata":{"id":"8bcduWsAbCRl"},"source":["# Here are our inputs.\n","X = np.array([[1, 3, -2, 0],\n","              [1, 1, 0, 1]])\n","Y = np.array([0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_tdGfEoDovBm"},"source":["Let's write out our model function:\n","\n","\\begin{align}\n","h_W(x) = \\phi(w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3) = \\phi(xW^T) = \\frac{1}{1+e^{(-xW^T)}}\n","\\end{align}\n","\n","We can get all predictions with this matrix product:\n","\n","\\begin{align}\n","\\hat{Y} = h_W(X) = \\phi(XW^T) =\n","\\phi\\begin{pmatrix}\n","x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n","x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n","\\vdots & \\vdots & \\vdots & \\vdots \\\\\n","x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","w_0 \\\\\n","w_1 \\\\\n","w_2 \\\\\n","w_3 \\\\\n","\\end{pmatrix}\n","\\end{align}\n","\n","First let's write the sigmoid (logistic) function $\\phi$."]},{"cell_type":"code","metadata":{"id":"Hpah13BcCVXo"},"source":["def sigmoid(z):\n","  return 1 / (1 + np.exp(-z))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KLh6VWUfGm7_"},"source":["Now, given some initial parameter values (below), compute the model's initial predictions."]},{"cell_type":"code","metadata":{"id":"pGg1Ll4I4jR6"},"source":["# Initial parameter values.\n","W = [1, 1, 1, 1]\n","\n","# Compute predictions.\n","preds = sigmoid(np.dot(X, W))\n","print(preds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIbpXB4ZHPvO"},"source":["We're not going to use MSE for logistic regression. Instead, we'll use the *logistic loss*, also called *binary cross-entropy* (more on that name later):\n","\n","\\begin{align}\n","LogLoss = \\frac{1}{m} \\sum_i -y_i\\log(\\hat{y_i}) - (1-y_i)\\log(1-\\hat{y_i})\n","\\end{align}\n","\n","Despite this new loss function, it turns out that the gradient computation is the same as it was for MSE with linear regression. A happy coincidence.\n","\n","\\begin{align}\n","\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)X\n","\\end{align}\n","\n","Let's write the code for a single gradient descent step:"]},{"cell_type":"code","metadata":{"id":"Zl_Nu_wB8ar4"},"source":["# Run gradient descent\n","m, n = X.shape  # m = number of examples; n = number of features (including bias)\n","learning_rate = 0.1\n","\n","preds = sigmoid(np.dot(X, W))\n","loss = (-Y * np.log(preds) - (1 - Y) * np.log(1 - preds)).mean()\n","gradient = np.dot((preds - Y), X) / m\n","W = W - learning_rate * gradient\n","\n","print('predictions:', preds)\n","print('loss:', loss)\n","print('gradient:', gradient)\n","print('weights:', W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMsSJ-ZD_12e"},"source":["## Now with TensorFlow/Keras"]},{"cell_type":"code","metadata":{"id":"jisaFtGY__KL"},"source":["tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Dense(\n","    units=1,                     # output dim\n","    input_shape=[4],             # input dim\n","    use_bias=False,              # we included the bias in X\n","    activation='sigmoid',        # apply a sigmoid to the output\n","    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n","))\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n","model.compile(loss='binary_crossentropy', optimizer=optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PQ-RDwXCKVt"},"source":["# As above, get predictions for the current model first.\n","preds = model.predict(X)\n","\n","# Do a single gradient update.\n","history = model.fit(\n","  x = X,\n","  y = Y,\n","  epochs=1,\n","  batch_size=2,\n","  verbose=0)\n","\n","# Show the loss (before the update) and the new weights.\n","loss = history.history['loss'][0]\n","weights = model.layers[0].get_weights()[0].T\n","print('predictions:', preds.T)\n","print('loss:', loss)\n","print('W:', weights)"],"execution_count":null,"outputs":[]}]}